{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b0e6f0ac",
      "metadata": {},
      "source": [
        "## Applying Gaussian Mixture models to the NIST digits data\n",
        "\n",
        "We have already seen the NIST handwritten digits data from the perspective of classification.\n",
        "\n",
        "With classification, the digits were classified by labeling them as one of ten digits.\n",
        "\n",
        "The question arises: are the handwritten digits significantly different, to the extent of being able\n",
        "to find ten clusters that correspond to the ten digits 0,..,9."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fd02a73",
      "metadata": {
        "tags": [
          "ensureOutputDir"
        ]
      },
      "outputs": [],
      "source": [
        "dataDir = \"data\"\n",
        "# Make sure the outputDir subdirectory exists\n",
        "outputDir = \"output/GMM_KM\"\n",
        "import os, errno\n",
        "try:\n",
        "    os.makedirs(outputDir)\n",
        "except OSError as e:\n",
        "    if e.errno != errno.EEXIST:\n",
        "        raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0534192",
      "metadata": {
        "tags": [
          "loadData"
        ]
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_digits\n",
        "digits = load_digits()\n",
        "digits.data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b53e5870",
      "metadata": {},
      "source": [
        "Remember that each pixel in an image is a candidate feature, so there are 8\\*8 = 64 candidate attributes (features).\n",
        "\n",
        "Clustering in such high dimensional spaces can suffer from the curse of dimensionality.\n",
        "\n",
        "Therefore we see whether PCA can help to reduce the dimensions, while maintaining most of the information in the data.\n",
        "\n",
        "We apply a PCA decomposition with the intention of keeping 99% of the variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3094eb1",
      "metadata": {
        "title": "tags[\"fitPCA099\"]"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(0.99, whiten=True)\n",
        "data = pca.fit_transform(digits.data)\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77b54579",
      "metadata": {},
      "source": [
        "As can be seen, the transformed data has only 41 attributes (features), which is a significant reduction from 64. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35586331",
      "metadata": {
        "tags": [
          "plotAICperComponent"
        ]
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt \n",
        "from sklearn.mixture import GaussianMixture\n",
        "import numpy as np\n",
        "n_components = np.arange(50, 210, 10)\n",
        "models = [GaussianMixture(n, covariance_type='full', random_state=0)\n",
        "          for n in n_components]\n",
        "aics = [model.fit(data).aic(data) for model in models]\n",
        "plt.plot(n_components, aics)\n",
        "frame = plt.gca()\n",
        "frame.axes.get_yaxis().set_visible(False)\n",
        "plt.savefig(outputDir + '/digitsPCAtransformedNumComponentsAIC.pdf')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11119bf2",
      "metadata": {},
      "source": [
        "As can be seen, there appear to be 150 components in the 'best' model according to the AIC (Akaike Information Criterion - a model validation metric that attempts to balance adding more terms against the danger of overfitting the training data) metric. We duly try a Gaussian Mixture model fit with 150 components and check that the EM algorithm converged in that case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9e286d6",
      "metadata": {
        "tags": [
          "fitGMM150"
        ]
      },
      "outputs": [],
      "source": [
        "gmm = GaussianMixture(150, covariance_type='full', random_state=0)\n",
        "gmm.fit(data)\n",
        "print(gmm.converged_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "446170eb",
      "metadata": {},
      "source": [
        "Since it converged, we generate a sample of 100 points from the 150 _cluster density functions_ of handwritten digits. Note that these sample points correspond to (transformed) handwritten digits, but are extremely unlikely to be identical to any of the handwritten digits used to derive the cluster regions/probability functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e14716e",
      "metadata": {
        "tags": [
          "sampleGMM100"
        ]
      },
      "outputs": [],
      "source": [
        "X_new, y_new = gmm.sample(100)\n",
        "X_new.shape, y_new[:,None].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccfc6695",
      "metadata": {},
      "source": [
        "As you can see, `X_new` has 100 of the 41-attribute transformed handwritten digits. We apply the inverse transformation to return the data to 8\\*8 pixel space and inspect the resulting images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32cac37d",
      "metadata": {
        "tags": [
          "plotDigits"
        ]
      },
      "outputs": [],
      "source": [
        "import clusSupport\n",
        "digits_new = pca.inverse_transform(X_new)\n",
        "clusSupport.plot_digits(digits_new)\n",
        "plt.savefig(outputDir + '/digitsGenerated110Components.pdf')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d4e4ae2",
      "metadata": {},
      "source": [
        "As can be seen these are plausible handwritten digits, most of which are recognisable. Each was sampled from a region of one of the 110 Gaussian density functions, and was almost centainly not submitted as an image of an actual handwritten digit. Thus clustering can be used in this way to generate new instances, based on _clustered exemplars_.\n",
        "\n",
        "One of the difficulties with classification is the need to label the instances in the training set. This is usually a laborious operation, with lots of human intervention. As we saw above, clustering can do a surprisingly good job even without labels. The natural question is: can clustering derive the labels itself?\n",
        "\n",
        "To answer this question, we use KMeans to assign the handwritten digits to exactly 10 clusters, in the hope that these clusters might coincide with the 10 labels 0,1,..,9."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eda939b1",
      "metadata": {
        "tags": [
          "kmeans10"
        ]
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "kmeans = KMeans(n_clusters=10, random_state=0)\n",
        "clusters = kmeans.fit_predict(digits.data)\n",
        "kmeans.cluster_centers_.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f542e21",
      "metadata": {},
      "source": [
        "Now we take the centres of each of the 10 clusters, interpret them as pixel intensities and display them as images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a09f04f",
      "metadata": {
        "tags": [
          "plotKmeansClusters"
        ]
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(2, 5, figsize=(8, 3))\n",
        "centers = kmeans.cluster_centers_.reshape(10, 8, 8)\n",
        "for axi, center in zip(ax.flat, centers):\n",
        "    axi.set(xticks=[], yticks=[])\n",
        "    axi.imshow(center, interpolation='nearest', cmap=plt.cm.binary)\n",
        "plt.savefig(outputDir + '/digitsClusterCentres10.pdf')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13676276",
      "metadata": {},
      "source": [
        "Amazingly, most of the KMeans cluster centres look like recognisable digits. We assume that each cluster centre is intended to match a particular digit, probably the most common of the labels used with the associated observation in the cluster. For example, if the cluster centre looks like '4', most of the instances that KMeans assigned to that cluster should have been labeled as 4 in the training data. Those that were not labeled as 4 but have been assigned to that cluster might have been assigned incorrectly by KMeans."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5706538",
      "metadata": {
        "tags": [
          "applyMode"
        ]
      },
      "outputs": [],
      "source": [
        "from scipy.stats import mode\n",
        "\n",
        "labels = np.zeros_like(clusters)\n",
        "for i in range(10):\n",
        "    mask = (clusters == i)\n",
        "    labels[mask] = mode(digits.target[mask])[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5697f9fb",
      "metadata": {
        "tags": [
          "classificationMetrics"
        ]
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(digits.target, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9a5a40e",
      "metadata": {},
      "source": [
        "Even though KMeans did not use any labels, it assigned approximately 80% of handwritten digits to the correct cluster, which is far better than might be expected by chance. Note that interpreting cluster membership is outside the scope of a clustering algorithm like KMeans."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d723a1eb",
      "metadata": {
        "tags": [
          "confusionMatrix"
        ]
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "mat = confusion_matrix(digits.target, labels)\n",
        "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
        "            xticklabels=digits.target_names,\n",
        "            yticklabels=digits.target_names)\n",
        "plt.xlabel('true label')\n",
        "plt.ylabel('predicted label')\n",
        "plt.savefig(outputDir + '/digitsClusterConfusionMatrix10.pdf')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cab62ab",
      "metadata": {},
      "source": [
        "The confusion matrix above indicates where any misclassifications arise. Generally, cluster assignment and classification are very close except for 1's and 8's, which is consistent with the cluster centre images above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5da1693",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
