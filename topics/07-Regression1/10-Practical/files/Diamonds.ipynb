{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2c98034e",
      "metadata": {},
      "source": [
        "# Diamond Data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7463477",
      "metadata": {},
      "source": [
        "As an example, we access some available diamond data: prices in Singapore dollars and weights in carats (the standard measure of diamond mass, equal to 0.2 g). The diamond data can be downloaded from [the Journal of Statistics Education](http://jse.amstat.org/jse_data_archive.htm) and placed in the data sub directory as 'data/diamond.dat.txt')."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2432450c",
      "metadata": {},
      "source": [
        "Unlike [R](https://www.r-project.org/about.html), Python is a general-purpose language and does not have built-in features for data mining, machine learning etc. Instead, Python library developers took the core features of R (which is open source) and re-implemented them so that they can be used as function calls in Python. One of the main libraries they developed was [`pandas`](https://pandas.pydata.org/) which provides many of the same data-handling functionality of R, notably dataframes, lists, arrays, etc., with very similarly semantics to their R equivalents. This library needs to be imported first, and is given an alias `pd` for convenience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a39747d8",
      "metadata": {
        "tags": [
          "importData"
        ]
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "dataFile = \"data/diamond.dat.txt\"\n",
        "#import os\n",
        "#if not os.path.exists('res'):\n",
        "#  os.makedirs('res')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9885286",
      "metadata": {},
      "source": [
        "The next step is to read the data into the `diamondData` variable. Relative to this notebook, it can be found at `dataFile`. Whitespace is used to delimit columns in each record. The file does not include a header line with column names, so we add them on input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa7f5b33",
      "metadata": {
        "tags": [
          "readDara"
        ]
      },
      "outputs": [],
      "source": [
        "diamondData = pd.read_csv(dataFile, sep='\\s+', header=None, names=[\"carats\",\"price\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d22ab1e",
      "metadata": {},
      "source": [
        "Having read the data, it is a good idea to view the leading rows, just to see its structure, etc. Note that, as with R, this is just a method call on the object (`diamondData` in this instance)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d691cb23",
      "metadata": {
        "tags": [
          "diamondHead"
        ]
      },
      "outputs": [],
      "source": [
        "diamondData.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "136b2d29",
      "metadata": {},
      "source": [
        "It is also a good idea to plot the data, to see what it looks like. Python offers the [`matplotlib`](https://matplotlib.org/) library. We provide a directive that any plots should be put inline rather than in a separate window and then import `matplotlib` and assign an alias of `plt` to this library for convenience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47d64c4a",
      "metadata": {
        "tags": [
          "importMatplotlib"
        ]
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56ad1009",
      "metadata": {},
      "source": [
        "We create a figure object and then add plot elements to it. The most component is the scatterplot generated by the `ax.scatter()` call. We can access the variables using the format `dataframe.column`. The AxesSubPlot object is just one plot ($1 \\times  1$) in this case. Axis labels and a title can be added using method calls."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62f30054",
      "metadata": {
        "tags": [
          "plotData"
        ]
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(1,1,1)\n",
        "\n",
        "ax.scatter(diamondData.carats, diamondData.price)\n",
        "fig.suptitle(\"Relation between diamonds' price and weight\")\n",
        "ax.set_ylabel('Price [SIN $]')\n",
        "ax.set_xlabel('Weight [carat]')\n",
        "ax.grid(True)\n",
        "#resFig = \"res/diamonds.pdf\"\n",
        "#plt.savefig(resFig)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15f3f80b",
      "metadata": {},
      "source": [
        "## Fit a model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6007335",
      "metadata": {},
      "source": [
        "We are going to assume that diamond prices increase linearly with their weight. Having learned the parameters $ \\beta_0, \\beta_1 $ of that relationship, we can use them to predict the price for any weight (even one we have not seen in the training set).\n",
        "\n",
        "Python offers the [`sklearn`](http://scikit-learn.org/stable/) library for machine learning operations, such as linear regression. However, sometimes it is helpful to use it through a more high-level \"facade\". In this regard, [`statsmodel`](https://github.com/statsmodels/statsmodels) provides a nice way to specify models and interact with them in a more R-like (and arguably, in a more \"natural\") way than is possible with `sklearn`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98dedc5b",
      "metadata": {
        "tags": [
          "importSM"
        ]
      },
      "outputs": [],
      "source": [
        "import statsmodels.api as sm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9ef1537",
      "metadata": {},
      "source": [
        "We will use its function *OLS()* (Ordinary Least Squares) that fits a linear regression based on the Ordinary Least Squares algorithm.  \n",
        "\n",
        "The model we want to get is : $\\hat{y} = \\beta_0 + \\beta_1 X$.\n",
        "\n",
        "where $\\hat{y}$ is the estimated diamond Price (the dependent variable) and $x$ is the diamond Weight (the independent variable, called `carats` in the data).  \n",
        "\n",
        "An intercept is not included by default and should be added when creating the $X$ matrix above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3720dc1f",
      "metadata": {
        "tags": [
          "SMaddConst"
        ]
      },
      "outputs": [],
      "source": [
        "X = sm.add_constant(diamondData.carats)\n",
        "X.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f223108b",
      "metadata": {},
      "source": [
        "Fitting the data (minimising the residual sum of squares) can be done easily using the `fit()` method of `statsmodel.OLS()`. The fit parameters themselves $\\beta_0$ and $\\beta_1$ can be extracted using the "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bce287c0",
      "metadata": {
        "tags": [
          "simpleModel"
        ]
      },
      "outputs": [],
      "source": [
        "simpleModel = sm.OLS(diamondData.price, X).fit()\n",
        "simpleModel.params  # here are the beta coefficients (intercept and slope of the linear regression line)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d62b793e",
      "metadata": {
        "variables": {
          "simpleModel.params.carats": "3721.0248515504727",
          "simpleModel.params.const": "-259.62590719155486"
        }
      },
      "source": [
        "The intercept ($\\beta_0 = $ {{simpleModel.params.const}}) and the slope ($\\beta_1 = ${{simpleModel.params.carats}}). Thus our model is\n",
        "\n",
        "$ \\hat{y} = ${{simpleModel.params.const}} + {{simpleModel.params.carats}} $x$\n",
        "\n",
        "where $x$ is the specific diamond Weight in carats and $y$ is the predicted diamond Price in Singapore dollars."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51c3147c",
      "metadata": {},
      "source": [
        "We can plot the obtained regression line together with the input data X.\n",
        "We can do it by drawing a line using the beta parameters just calculated or also plotting the fitted values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c23d9e1",
      "metadata": {
        "tags": [
          "plotSMdata"
        ]
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(1,1,1)\n",
        "\n",
        "ax.scatter(diamondData.carats, diamondData.price)\n",
        "# draw linear regression line\n",
        "x = [0.1,0.4]\n",
        "y = [simpleModel.params.const + simpleModel.params.carats * i for i in x] \n",
        "ax.plot(x, y)\n",
        "# Plot the fitted values as \"orange\" dots for comparison with the \"blue\" data dots \n",
        "y_hat = simpleModel.fittedvalues\n",
        "ax.scatter(diamondData.carats, y_hat)\n",
        "# Add a title and labels to the plot\n",
        "fig.suptitle(\"Relationship between diamonds' price and weight, with OLS fit\")\n",
        "ax.set_ylabel('Price [SIN $]')\n",
        "ax.set_xlabel('Weight [carat]')\n",
        "ax.grid(True)\n",
        "#plt.savefig(\"res/diamondsFit.pdf\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37bb5683",
      "metadata": {},
      "source": [
        "## Analyse the model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f523af92",
      "metadata": {},
      "source": [
        "### Coefficients interpretation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b7eb845",
      "metadata": {},
      "source": [
        "$\\beta_1$ (the slope of the regression line) is the __expected change in response for a 1 unit change in the predictor.__  \n",
        "In our case, we expect 3721 Singapore dollars increase in price for every carat increase in mass of diamond.  \n",
        "This applies is within the restricted range considered; extrapolation of the regression line for bigger diamond stones would not be advisable as these gems are rarer and command a different price range.\n",
        "\n",
        "$\\beta_0$ (the intercept of the regression line) is the **expected price when the weight is zero.**\n",
        "This does not always make sense and in our case the negative intercept is even more puzzling because it suggests that a zero-carat diamond ring has a negative economic value!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64b54574",
      "metadata": {
        "tags": [
          "exerciseMeanCentre"
        ]
      },
      "source": [
        "### Exercise: Centre the data by the mean and re-fit\n",
        "\n",
        "Since we do not have any data on the price of diamonds with very low weights, the intercept has little meaning here. However, the intercept at the mean price would have a meaning: it is the price of diamonds at the mean weight in the sample. __You are asked to mean-centre the weight data and obtain the OLS fit to this mean-centred data.__ Hint: the centre of the weight data can be calculated using `diamondData.carats.mean()`.\n",
        "\n",
        "__How do the $\\beta$ parameters for the mean-centred data differ from those of the original data?__"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19e096a5",
      "metadata": {},
      "source": [
        "## Predicting the price of a diamond"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69092b38",
      "metadata": {},
      "source": [
        "Once we have a model of the relation, we can use it for predictions.  `statsmodel` provides a `predict()` method for this purpose.\n",
        "\n",
        "For a single prediction, say a diamond weighing 0.2 carats, we can use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23640389",
      "metadata": {
        "tags": [
          "SMpredict"
        ]
      },
      "outputs": [],
      "source": [
        "newDiamond = [1, 0.2] # remember to add the intercept term (1)!\n",
        "simpleModel.predict(newDiamond)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "242855ed",
      "metadata": {},
      "source": [
        "For multiple weights, we create an object with a row for each weight to be estimated. Note that the row needs to include the intercept term."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84643651",
      "metadata": {},
      "outputs": [],
      "source": [
        "newDiamonds = sm.add_constant([0.16, 0.27, 0.34]) # add the intecept for each of the 3 weights\n",
        "simpleModel.predict(newDiamonds)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09359212",
      "metadata": {},
      "source": [
        "Result: for 0.16, 0.27, and 0.34 carats, we predict the prices to be 335.74, 745.05, 1005.52 (SIN) dollars"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dd52120",
      "metadata": {},
      "source": [
        "## Residuals"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ac5f59c",
      "metadata": {},
      "source": [
        "As we have seen previously, the residuals are the difference between the observed (y) and the predicted outcome (y_hat). Alternatively, they can be obtained directly from the model, using, say `simpleModel.resid`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f421c4f",
      "metadata": {},
      "source": [
        "### Exercise: Plot the residuals and look for patterns, trends or biases\n",
        "\n",
        "Using what you have learnt above, in relation to plotting, plot the residuals and check the following\n",
        "\n",
        "1. they are centred on zero\n",
        "2. there are no obvious patterns, e.g., runs of positive residuals followed by runs of negative residuals\n",
        "3. the variance of the residuals is roughly constant over the range of prices\n",
        "4. most of the residuals are near zero, but there might be a small number of more extreme values\n",
        "4. there are no obvious outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "154aec6e",
      "metadata": {
        "tags": [
          "plotSMresid"
        ]
      },
      "outputs": [],
      "source": [
        "residuals = simpleModel.resid\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(1,1,1)\n",
        "\n",
        "ax.scatter(diamondData.carats, residuals)\n",
        "fig.suptitle(\"Residuals plotted against weight\")\n",
        "ax.set_ylabel('Actual - Prediced Price [SIN $]')\n",
        "ax.set_xlabel('Weight [carat]')\n",
        "ax.grid(True)\n",
        "#resFig = \"res/residuals.pdf\"\n",
        "#plt.savefig(resFig)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdb49464",
      "metadata": {},
      "source": [
        "One limitation of the diamonds data is that it has only one feature (`carats`) that is available for prediction.\n",
        "To make the data more interesting(!), I am going to manufacture an additional feature, which I am going to call `quality`.\n",
        "To do this, I have decided to take the residuals from the fit to one feature, and scale it so that it is between\n",
        "-1 and 1, and to use the resulting data as if it was a feature provided in the original dataset.\n",
        "Of course I am cheating here, but it is just so that we can compare models, to help your learning.\n",
        "___Do NOT do this with real data!!___"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b791f98",
      "metadata": {
        "tags": [
          "diamond2"
        ]
      },
      "outputs": [],
      "source": [
        "# Create a (cloned) copy of our diamondData dataframe.\n",
        "# That way we can make changes to the clone, without affecting the original diamondData dataframe.\n",
        "diamondData2 = diamondData.copy()\n",
        "\n",
        "# We insert the new \"quality\" column\n",
        "diamondData2.insert(1,'quality',residuals,False)\n",
        "\n",
        "# We then proceed to scale the column using scikit-learn's `MinMaxScaler`, which scales the data in the quality column\n",
        "# so that the most negative residual(s) become -1 and the most positive residual(s) become 1, with the remaining residuals\n",
        "# ending up somewhere in between the two extremes.\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "diamondData2[['quality']] = scaler.fit_transform(diamondData2[[\"quality\"]]).round(1)\n",
        "diamondData2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "041de6e3",
      "metadata": {
        "tags": [
          "simpleModel2"
        ]
      },
      "outputs": [],
      "source": [
        "# Now we can derive the new features dataframe `X2` and apply OLS \n",
        "X2 = sm.add_constant(diamondData2[[\"carats\",\"quality\"]])\n",
        "simpleModel2 = sm.OLS(diamondData2.price, X2).fit()\n",
        "simpleModel2.params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad975761",
      "metadata": {
        "tags": [
          "plotSM2resid"
        ]
      },
      "outputs": [],
      "source": [
        "residuals2 = simpleModel2.resid\n",
        "fig = plt.figure()\n",
        "# We plot the residuals of the new model fitted to the training data\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "ax.scatter(diamondData2.carats, diamondData2.quality, residuals2)\n",
        "fig.suptitle(\"Residuals plotted against weight and quality\")\n",
        "ax.set_zlabel('Actual - Prediced Price [SIN $]')\n",
        "ax.set_ylabel('Quality [ratio]')\n",
        "ax.set_xlabel('Weight [carat]')\n",
        "ax.grid(True)\n",
        "#resFig = \"res/residuals2.pdf\"\n",
        "#plt.savefig(resFig)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1d6504a",
      "metadata": {
        "tags": [
          "plotSM2resid2D"
        ]
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "\n",
        "ax1 = plt.subplot(1, 2, 1)\n",
        "ax1.scatter(diamondData2.carats, residuals2)\n",
        "ax1.set_ylabel('Actual - Prediced Price [SIN $]')\n",
        "ax1.set_xlabel('Weight [carat]')\n",
        "\n",
        "ax2 = plt.subplot(1, 2, 2)\n",
        "ax2.scatter(diamondData2.quality, residuals2)\n",
        "ax2.set_ylabel('Actual - Prediced Price [SIN $]')\n",
        "ax2.set_xlabel('Quality [ratio]')\n",
        "\n",
        "fig.suptitle(\"Residuals plotted against weight\")\n",
        "plt.tight_layout()\n",
        "#resFig = \"res/residuals3.pdf\"\n",
        "#plt.savefig(resFig)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "070410c9",
      "metadata": {},
      "source": [
        "### Residuals should be normally distributed"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd09dd2a",
      "metadata": {},
      "source": [
        "The mean of the residuals is expected to be zero, as we can check below "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d09497b4",
      "metadata": {
        "tags": [
          "residMean"
        ]
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "residuals = simpleModel.resid\n",
        "residMean = np.mean(residuals)\n",
        "resid2Mean = np.mean(residuals2)\n",
        "[residMean, resid2Mean]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1073be1b",
      "metadata": {},
      "source": [
        "The distribution of the residuals should be approximately Normal (sometimes called Gaussian) distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2be8acb9",
      "metadata": {
        "tags": [
          "residHist"
        ]
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "plot = sns.displot(x = residuals, kde=True)\n",
        "#resFig = \"res/residHist.pdf\"\n",
        "#plot.savefig(resFig)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6236ecc0",
      "metadata": {
        "tags": [
          "residHist2"
        ]
      },
      "outputs": [],
      "source": [
        "plot = sns.displot(x = residuals2, kde=True)\n",
        "#resFig = \"res/residHist2.pdf\"\n",
        "#sns_plot.savefig(resFig)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86b79d2a",
      "metadata": {},
      "source": [
        "Q-Q plots (stands for a \"quantile-quantile plot\") can be used to check whether the data is distributed Normally or not.  \n",
        "\n",
        "It is a plot where the axes are transformed so that a Normal (or Gaussian) distribution appears in a straight line. In other words, a __perfectly Normal distribution would exactly follow a line with slope = 1 and intercept = 0.__\n",
        "\n",
        "If the plot does not appear to be - roughly - a straight line, then the underlying distribution is not normal. If it bends down at the left and up at the right, then there are more extreme values than expected.\n",
        "\n",
        "The theoretical quantiles are placed along the x-axis. That is, the x-axis is not your data, it is simply an expectation of where your data should have been, if it were Normal.\n",
        "\n",
        "The actual data is plotted along the y-axis. The values are the standard deviations from the mean. So, 0 is the mean of the data (residuals in this case), 1 is 1 standard deviation above, etc. This means, for instance, that 68.27% of all your data should be between -1 & 1, if you have a normal distribution.\n",
        "\n",
        "`statsmodels` offers a handy `qqplot()` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16e9bf31",
      "metadata": {
        "tags": [
          "residualsQQ"
        ]
      },
      "outputs": [],
      "source": [
        "# Q-Q plot to verify the residuals distribution\n",
        "plot = sm.qqplot(residuals, fit=True, line = '45')\n",
        "#resFig = \"res/residualsqq.pdf\"\n",
        "#fig.savefig(resFig)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae4643c4",
      "metadata": {
        "tags": [
          "residuals2QQ"
        ]
      },
      "outputs": [],
      "source": [
        "plot = sm.qqplot(residuals2, fit=True, line = '45')\n",
        "#resFig = \"res/residuals2qq.pdf\"\n",
        "#fig.savefig(resFig)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cc8a96c",
      "metadata": {},
      "source": [
        "Note that the QQ plot for `residuals2` agrees with the histogram plot: the residual distribution is not Gaussian (normal) as it was for `residuals`. That is due to the way the additional (quality) variable was generated (in such a way that the distribution of the residuals is expected to be more uniform than normal, because of rounding effects).\n",
        "\n",
        "Another thing to check is to look for observatons that have a significant influence on the model, especially those with a high value of the residual. If such an observation was an outlier, its removal would have a larger effect on the ultimate fitted model.\n",
        "\n",
        "Interestingly, both the `simple` and `simple2` models seem to agree that observation #41 has a lot of leverage on the fit, and the size of its residual with respect to the `simple` suggests it might be an outlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7185a44",
      "metadata": {
        "tags": [
          "smInfluence"
        ]
      },
      "outputs": [],
      "source": [
        "# Statsmodels provides an `influence` plot that we can use to check the influence of a each observation\n",
        "# This can help to diagnose problems with specific observations, but can become unwieldy if there are many\n",
        "# training set observations.\n",
        "fig = sm.graphics.influence_plot(simpleModel, criterion=\"cooks\")\n",
        "fig.tight_layout(pad=1.0)\n",
        "#resFig = \"res/simpleModel_influence.pdf\"\n",
        "#fig.savefig(resFig)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3312a2b5",
      "metadata": {
        "tags": [
          "sm2Inflouence"
        ]
      },
      "outputs": [],
      "source": [
        "fig = sm.graphics.influence_plot(simpleModel2, criterion=\"cooks\")\n",
        "fig.tight_layout(pad=1.0)\n",
        "#resFig = \"res/simpleModel2_influence.pdf\"\n",
        "#fig.savefig(resFig)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e50ebbf1",
      "metadata": {
        "tags": [
          "smCarats4"
        ]
      },
      "outputs": [],
      "source": [
        "# We can also plot the model prediction against individual features\n",
        "fig = sm.graphics.plot_regress_exog(simpleModel, \"carats\")\n",
        "fig.tight_layout(pad=1.0)\n",
        "#resFig = \"res/simpleModel_carats_4plot.pdf\"\n",
        "#fig.savefig(resFig)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb5d7949",
      "metadata": {
        "tags": [
          "sm2carats4"
        ]
      },
      "outputs": [],
      "source": [
        "fig = sm.graphics.plot_regress_exog(simpleModel2, \"carats\")\n",
        "fig.tight_layout(pad=1.0)\n",
        "#resFig = \"res/simpleModel2_carats_4plot.pdf\"\n",
        "#fig.savefig(resFig)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ef27da9",
      "metadata": {
        "tags": [
          "sm2quality4"
        ]
      },
      "outputs": [],
      "source": [
        "fig = sm.graphics.plot_regress_exog(simpleModel2, \"quality\")\n",
        "fig.tight_layout(pad=1.0)\n",
        "#resFig = \"res/simpleModel2_quality_4plot.pdf\"\n",
        "#fig.savefig(resFig)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c202ae7f",
      "metadata": {},
      "source": [
        "It is clear that the additional model feature `quality` does help the regression: `price` and its `fitted` value are closer, residuals show no remaining trends, etc.\n",
        "\n",
        "This shows that the model has taken up most of the signal in the data and what is left is just \"noise\"."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f36d3aeb",
      "metadata": {
        "tags": [
          "smTrainTest"
        ]
      },
      "source": [
        "__Exercise__ Use a training-test split with each of `simpleModel` and `simpleModel2` and decide whether it is worth adding the `quality` feature in the model.\n",
        "Note that, to create the quality feature, you will need to fit the entire dataset first, before splitting it."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd12795d",
      "metadata": {},
      "source": [
        "## Measures of fit computed from the residuals "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46d2d2f1",
      "metadata": {},
      "source": [
        "The residual variation measures how well the regression line fits the data points.  \n",
        "\n",
        "It is the variation in the dependent variable (Price) that is not explained by the regression model and is represented by the residuals. We want the residual variation to be as small as possible.  \n",
        "\n",
        "Each residual is distributed normally with mean 0 and variance = $\\sigma^2$.    \n",
        "\n",
        "The __Root Mean Squared Error (RMSE)__ is defined as follows:  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b37f454",
      "metadata": {
        "tags": [
          "smRMSE"
        ]
      },
      "outputs": [],
      "source": [
        "y = diamondData.price\n",
        "n = len(y)\n",
        "p = 2\n",
        "df = n-p\n",
        "MSE = sum(residuals**2) / df\n",
        "RMSE = np.sqrt(MSE)\n",
        "RMSE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2a09947",
      "metadata": {},
      "source": [
        "RMSE can be used to calculate the standardized residuals too.  \n",
        "\n",
        "Large standardized residuals indicate extreme values, some of which might be outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "993b9b72",
      "metadata": {
        "tags": [
          "stdResid"
        ]
      },
      "outputs": [],
      "source": [
        "standardisedResiduals = simpleModel.resid / RMSE\n",
        "upperExtreme = max(standardisedResiduals)\n",
        "lowerExtreme = min(standardisedResiduals)\n",
        "[lowerExtreme, upperExtreme]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c93827be",
      "metadata": {},
      "source": [
        "### Summarizing the variation: R-squared"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68500ba1",
      "metadata": {},
      "source": [
        "The total variation is the residual variation (variation after removing predictors) plus the systematic variation (variation explained by regression model).  \n",
        "\n",
        "**R-squared** is the percentage of variability explained by the regression model:  \n",
        "\n",
        "R-squared = explained / total variation = 1 - residual / total variation\n",
        "\n",
        "R-squared is always between 0 and 1 (0% and 100%):\n",
        "- 0% indicates that the model explains none of the variability of the response data around its mean.\n",
        "- 100% indicates that the model explains all the variability of the response data around its mean.  \n",
        "\n",
        "In general, the higher the R-squared, the better the model fits your data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b52bf905",
      "metadata": {
        "tags": [
          "smRsq"
        ]
      },
      "outputs": [],
      "source": [
        "simpleModel.rsquared"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0c3c61e",
      "metadata": {},
      "source": [
        "R-squared can be a misleading summary and needs to be carefully taken (deleting data can inflate R-squared for example). Because of this, sometimes is preferred to use the **adjusted Rsquared**, which is Rsquared adjusted for the number of observations.  There are several formulas that can be used, such as Wherry's formula:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73e238f4",
      "metadata": {},
      "outputs": [],
      "source": [
        "1 - (1-simpleModel.rsquared)*((n-1)/simpleModel.df_resid)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bddf1965",
      "metadata": {},
      "source": [
        "Of course, it is also available from the model results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0fee056",
      "metadata": {
        "tags": [
          "smRsqAd"
        ]
      },
      "outputs": [],
      "source": [
        "simpleModel.rsquared_adj"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9d54a3a",
      "metadata": {},
      "source": [
        "In conclusion, based on the residual distribution and Rsquared metric, the model is pretty good and the relation is very strong."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12254181",
      "metadata": {},
      "source": [
        "### Plot the confidence interval"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa71d86f",
      "metadata": {},
      "source": [
        "We calculate the interval for each x value; will use the isf() function to get the inverse survival function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2656537",
      "metadata": {},
      "outputs": [],
      "source": [
        "predicted = simpleModel.fittedvalues\n",
        "x_1 = simpleModel.model.exog # this is the observation matrix used to fit the data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fc0a763",
      "metadata": {},
      "source": [
        "Get the covariance matrix of the predictors (i.e., excluding the dependent variable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6adf572",
      "metadata": {},
      "outputs": [],
      "source": [
        "covMatrixForParams = simpleModel.cov_params()\n",
        "covMatrixForParams"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96e35921",
      "metadata": {},
      "source": [
        "By definition, $\\mbox{var}(\\hat{y}(x_0)) = \\mbox{diag}(\\sigma^2 x_0^T (X^TX)^{-1} x_0)$; see [this formula](https://www.otexts.org/1529). Note that $\\mbox{var}(\\beta) = \\sigma^2 (X^TX)^{-1}$ (see Equation 30 in [these notes](https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf)). Therefore we can substitute so that $ \\mbox{var}(\\hat{y}(x_0)) = \\mbox{diag}(x_0^T \\mbox{var}(\\beta) x_0)$ because $\\mbox{var}(\\beta) = \\sigma^2 (X^TX)^{-1}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e7e6a9d",
      "metadata": {},
      "outputs": [],
      "source": [
        "covMatrixForPredictedPts = np.matmul(x_1, np.matmul(np.array(covMatrixForParams), x_1.T))\n",
        "varPredictedPts = np.diagonal(covMatrixForPredictedPts)\n",
        "varPredictedLine = simpleModel.mse_resid\n",
        "totVarPredicted = varPredictedLine + varPredictedPts\n",
        "totSePredicted = np.sqrt(totVarPredicted)\n",
        "totSePredicted"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0bba30e",
      "metadata": {
        "variables": {
          "simpleModel.df_resid": "46.0"
        }
      },
      "source": [
        "Lookup the value of the $t$ statistic associated with $\\alpha = 0.025$, using the degres of freedom from the fit = {{simpleModel.df_resid}}. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c07b9fda",
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.stats import t\n",
        "alpha=0.05 # confidence interval for two-sided hypothesis\n",
        "qt = 1 - (alpha/2)  # (0.25, 0.975) for a 2-sided 95% probability\n",
        "\n",
        "tppf = t.isf(alpha/2.0, simpleModel.df_resid)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "319bc46d",
      "metadata": {},
      "source": [
        "Now compute the bounding confidence limits as (predicted - tppf\\*totSePredicted, predicted + tppf\\*totSePredicted). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31e598c4",
      "metadata": {},
      "outputs": [],
      "source": [
        "interval_u = predicted + tppf * totSePredicted\n",
        "interval_l = predicted - tppf * totSePredicted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a464ce8b",
      "metadata": {
        "tags": [
          "smConfInt"
        ]
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "fig, ax = plt.subplots()\n",
        "x = diamondData.carats\n",
        "ax.plot(x,y, 'o', label=\"data\")\n",
        "ax.plot(x, simpleModel.fittedvalues, 'g-', label=\"OLS\")\n",
        "\n",
        "ax.plot(x, interval_u, 'c--', label = \"Intervals\")\n",
        "ax.plot(x, interval_l, 'c--')\n",
        "\n",
        "# Provide labels etc.\n",
        "fig.suptitle(\"OLS Linear Regression with confidence intervals\")\n",
        "ax.set_ylabel('Predicted Price [SIN $]')\n",
        "ax.set_xlabel('Weight [Carat]')\n",
        "ax.grid(True)\n",
        "ax.legend(loc='best')\n",
        "#resFig = \"res/smConfInt.pdf\"\n",
        "#plt.savefig(resFig)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "871da1f7",
      "metadata": {},
      "source": [
        "## Summary of statistic values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40686357",
      "metadata": {},
      "source": [
        "The *statsmodel* package offers an overview of the model values, similar to what we calculated above:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "510ef9b5",
      "metadata": {
        "tags": [
          "smSumm"
        ]
      },
      "outputs": [],
      "source": [
        "simpleModel.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4733f2c2",
      "metadata": {},
      "source": [
        "This table summarises the results of fitting this particular model to the data. It can be seen as having 4 sections, as follows:\n",
        "1. The problem as posed to statsmodels. The `y` variable is referred to as the \"dependent variable\" (`price`). The model we are fitting is Ordinary Least Squares`OLS`) with the `Method` being Least Squares. The number of rows in `X` is referred to as the `No. Observations`. The number of degrees of freedom in the residuals $epsilon$ is 46, because it is calculated as the tnumber of observations (48) - the number of terms in the model (2: the slope and intercept of the line). Recall the diagram in the lecture notes where there was a grey 'space' spanned by the 2 model parameters and a given $y$ value which lay outside that space.\n",
        "2. On the right hand side of this group you can see some metrics about the _overall_ fit. The F-statistic is a traditional measure of model performance - large values of the statisic, and small values of its probability are considered better. The are very encouraging here: $10^{-40}$  is effectively zero, meaning  the chances of the data happening by accident and not being generated by a model like this one are effectively zero too. The R-squared and adjusted R squared represent the ratio of the overall variance in the data that is explained by the model. Generally, anything above 0.9 is considered pretty good, so this value is very good. So it is often consiered a good idea to choose the model with the highest R-squared value. However, that can be misleading, because the \"better\" model (with the higher R-squared value) might overfit the data. Although they are less well-known, the _Akaike Information Criterion_ (`AIC`) and _Bayesian Information Criterion_ (`BIC`) are much better metrics to use, because they take account of the fact that models with more terms reduce the degrees of freedom and are more likely to overfit the training data, at the espense of leaving enough degrees of freedom to apply to test data. Generally, _lower_  values of AIC and BIC are considered better than larger values. Therefore, given a set of models, it is worth choosing the model with the _smallest_ value of AIC or BIC.\n",
        "3. Th next table applies to the fitted paramneters $\\beta$ (the slope and intercept of the line). The slope is referred to as `carats` here because that was the name of the feature we used in the model. A t-statistic is computed for each model parameter.  The most interesting feature is that its probability is so small in each case (effectively zero), suggesting that the chance that the true value of the model parameter is zero is effectively negligible. We can also see this from the fact that the confidence interval for each parameter does not include zero. The fact the fitted intercept is not zero might be surpriiong, because it suggests, for this data, that when the weight of the diamond is zero, the owner owes money! This is an artefact of the fact that it probably does not make sense to extrapolate this data down to very small diamonds. They might have their own relationship between weight and price. Extrapolation is generally  considered risk, because we are predicting values in regions of the features where we did not have any training data.\n",
        "4. The next block is a set of metrics relating to how well the data and model meet the underlying assumptions for linear regression. For example, we can check whether the skewness is approximately zero (i.e., the residuals are symmetric around zero) and that kurtosis (peakiness) of the observed residual distribution is close to the value of a normal distribution (3). They appear close enough in this case; the Jarque-Bera test is a test of normality based on skewness and kurtosis and looks good here. The Durbin-Watson statistic is a check on the sequence of residuals to see whether they show _serial correlation_ (which would be unwelcome), because it suggests there is something going on in the background like: the value of the next residual can be predicted from the value of the residual before it. This is not the case here, and the Durbin-Wason takes a value very near its expected value of 2, as required. So we know from these etrics that the residuals are close to being normal, and there is no evidence of serial correlation. Lastly,  the condition number is relatively small. This metric increases as the degree of independence between the features decreases. Typically condition numbers of $10^6$ and greater might be cause for concern. There is no concern here. Overall everything looks good for theis model-data combination.\n",
        "\n",
        "Commonly, such model summaries can be compared between models. We will see this when comparing model1 (without) and model2 (with) the `quality` feature."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dddf52a",
      "metadata": {},
      "source": [
        "Many values can also be accessed directly, for example the standard errors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11ca530c",
      "metadata": {},
      "outputs": [],
      "source": [
        "simpleModel.bse"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49b301de",
      "metadata": {},
      "source": [
        "You can see all the available values using `dir()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59f8e8e4",
      "metadata": {
        "tags": [
          "smObj"
        ]
      },
      "outputs": [],
      "source": [
        "dir(simpleModel)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bcf485e",
      "metadata": {},
      "source": [
        "You can also see a summary of the `simpleModel2` fit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a4c8802",
      "metadata": {
        "tags": [
          "sm2Summ"
        ]
      },
      "outputs": [],
      "source": [
        "simpleModel2.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4466452",
      "metadata": {},
      "source": [
        "Compared to the R-squared value of the simpler model, which was already acceptable, it has improved even more when the `quality` feature was added. This means that model2 explains more of the variability in the data than model1. But has this come at the expense of overfitting that data? The evidence from the reduced values of `AIC` and `BIC` are that it has not: model2 is better than model because its `AIC` and `BIC` values are better (smaller). Of course we also need to check other metrics, some of which have disimporved. For example the kurtosis value is not as good (we recall from the two distribution plots that this is apparent there too) but it does not seem to matter. This shows that it is sometimes possible to improve a model by adding a term, but without seeing an improvement across all metrics. Therefore, it is always advisable to supplement any such analysis with visualisation (as done above) and also to to use techniques like cross-validation to look for evidence of over- and under-fitting."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36b063fa",
      "metadata": {},
      "source": [
        "To summarise: look for:\n",
        "1. Larger is better: R-squared, Df residuals\n",
        "2. Smaller is better: Probabilities: F, t, Jarque-Bera; Balanced metrics: AIC, BIC; Solver difficulty: Condition number\n",
        "3. Should be close to ideal number: Skewness (0), Kurtosis (3), Durbin-Watson (2)\n",
        "\n",
        "You cannot always improve all metrics together, so some judgment is needed. __NB you should aways use visualisation (and cross-validation to check for overfitting) to back up any model building choices.__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7c0e343",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
