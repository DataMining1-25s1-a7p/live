{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7623fa6d",
      "metadata": {},
      "source": [
        "# Using Decision Trees: detailed example\n",
        "\n",
        "The _Play Tennis_ example data (from Mitchell (1997) _Machine Learning_) used in the notes for Week 8 is a nice, relatively small example.\n",
        "\n",
        "The predictors are discrete-valued labels, making the decision splits easier to understand.\n",
        "\n",
        "The first step is to read the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "686c4e82",
      "metadata": {
        "tags": [
          "load"
        ]
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import pandas as pd\n",
        "import pprint\n",
        "pp = pprint.PrettyPrinter(indent=2)\n",
        "dataDir = \"data\"\n",
        "df = pd.read_csv(dataDir+\"/playTennis.csv\",header=0,quotechar=\"'\")\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64ab7bbd",
      "metadata": {
        "tags": [
          "intro"
        ]
      },
      "source": [
        "For convenience, I have created a function to evaluate the _entropy_ function `H`.\n",
        "\n",
        "It looks more complex than it is - mainly because I assemble the calculation in a string, for teaching purposes. This string is returned to the caller and `eval`uated there by the calling program.\n",
        "\n",
        "Note that `H` has two forms, depending on whether it is Level 1 (applies to the overall set: arguments `predNameB` and `predLabelB` are each empty strings and `pd` evaluates to the number of rows in the overall set) or Level 2 (when it is conditional on a setting of one of the predictors, specified by `predNameB` and `predLabelB` and `pd` is also more specific).\n",
        "\n",
        "If a particular predictor setting does not arise for a given class setting, the row count `pn` is zero and the log term needs special treatment. The log of zero is minus infinity, but the log term itself is multipled by zero, so the two multiplied together is taken as zero.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea55c0a6",
      "metadata": {
        "tags": [
          "calcH"
        ]
      },
      "outputs": [],
      "source": [
        "def calcH(predNameB,predLabelB,pd):\n",
        "  acc = \"\"\n",
        "  for classLabel in labels[className]:\n",
        "    pns = \"rowCount[className]\"+predNameB+\"[classLabel]\"+predLabelB\n",
        "    pn = eval(pns)\n",
        "    p = \"(\"+str(pn) +\"/\" + str(pd)+\")\"\n",
        "    if (pn == 0):\n",
        "      acc += \" -\" + p + \" \"\n",
        "    else:\n",
        "      acc += \" -\" + p + \" * math.log(\" + p + \", 2) \"\n",
        "  return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3448e97",
      "metadata": {
        "tags": [
          "countsByAttribute"
        ]
      },
      "source": [
        "## Preliminary: Deriving the row counts\n",
        "\n",
        "We now start to count rows depending on various (combinations of) settings. We use a rowCount set to store the counts. The first dictionary element gets the count of `'ALL'` rows.\n",
        "\n",
        "We then get a list of the column names `colNames`and the names of the class to be predicted `className`.\n",
        "\n",
        "Looping over the column names, we ask what the unique list of labels is for each column name.\n",
        "\n",
        "We can then loop over each of these labels and count the number of rows for which this column name takes a particular label value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4c0b9a1",
      "metadata": {
        "tags": [
          "computeCounts"
        ]
      },
      "outputs": [],
      "source": [
        "rowCount = {}\n",
        "rowCount[\"ALL\"] = len(df.index)\n",
        "colNames = list(df.columns.values)\n",
        "className = \"play\"\n",
        "labels = {}\n",
        "for colName in colNames:\n",
        "  labels[colName] = df[colName].unique().tolist()\n",
        "  rowCount[colName] = {}\n",
        "  for label in labels[colName]:\n",
        "    rowCount[colName][label] = len(df.loc[df[colName] == label].index)\n",
        "pp.pprint(rowCount)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cdd118a",
      "metadata": {
        "tags": [
          "level2"
        ]
      },
      "source": [
        "For the Decision Tree classifier, we need to go to Level 2 also.\n",
        "\n",
        "Therefore we also need to split the predictor counts above based on whether the decision was to play or not.\n",
        "\n",
        "Our first step is to use a _list comprehension_ to return a list of the predictor column names only `predNames`.\n",
        "\n",
        "We can then loop over just the predictors.\n",
        "\n",
        "The resulting `rowCount` values depend on both the `className` = `classLabel` and the `predName` = `predLabel` conditions.\n",
        "\n",
        "We then print the rowCount variable and see that `rowCount` has been extended with these counts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7378738",
      "metadata": {
        "lines_to_next_cell": 2,
        "tags": [
          "calcLevel2"
        ]
      },
      "outputs": [],
      "source": [
        "predNames = [x for x in colNames if className not in x]\n",
        "for predName in predNames:\n",
        "  rowCount[className][predName] = {}\n",
        "  for classLabel in labels[className]:\n",
        "    rowCount[className][predName][classLabel] = {}\n",
        "    for predLabel in labels[predName]:\n",
        "      rowCount[className][predName][classLabel][predLabel] = len(df.loc[(df[className] == classLabel) & (df[predName] == predLabel)].index)\n",
        "pp.pprint(rowCount)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4aa76ca4",
      "metadata": {
        "tags": [
          "entropy"
        ]
      },
      "source": [
        "## Entropy calculations\n",
        "\n",
        "The top-level entropy calculation is calculated without any splits by predictor.\n",
        "\n",
        "It is calculated in terms of the class column (\"play\" in this case) which takes two values: \"yes\" and \"no\". As expected it takes a value based on how predictable this variable is.\n",
        "\n",
        "Note that the `calcH` function is called with empty strings for `predNameB`  and `predLabelB` and the `rowCount` for ALL settings.\n",
        "\n",
        "Also, the string returned by `calcH` needs to be evaluated to a number.\n",
        "\n",
        "We print both the `H` expression (as a string) and its value (as a number)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c26965df",
      "metadata": {
        "tags": [
          "calcEntropy"
        ]
      },
      "outputs": [],
      "source": [
        "H = {}\n",
        "H[className] = {}\n",
        "acc = calcH(predNameB=\"\",predLabelB=\"\",pd=rowCount[\"ALL\"])\n",
        "H[className][\"\"] = eval(acc)\n",
        "\n",
        "print(acc)\n",
        "print(H[className][\"\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92d608bd",
      "metadata": {
        "tags": [
          "split"
        ]
      },
      "source": [
        "We now consider the effects of splitting by each of the predictors in turn.\n",
        "\n",
        "In each case we scale by `p` which is the probability of a particular setting.\n",
        "\n",
        "The other term is of course the `H` for that setting with the class variable. Note that here `calcH` has `predNameB` and `predLabelB` that are not just empty strings.\n",
        "\n",
        "Using the `term` counter, we can tell whether we need to start a new accumulated string or just add to an existing accumulated string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c358588",
      "metadata": {
        "lines_to_next_cell": 2,
        "tags": [
          "calcSplit"
        ]
      },
      "outputs": [],
      "source": [
        "for predName in predNames:\n",
        "  term = 0\n",
        "  predNameB = '[\"'+predName+'\"]'\n",
        "  for predLabel in labels[predName]:\n",
        "    predLabelB = '[\"'+predLabel+'\"]'\n",
        "    p = \"(\" + str(rowCount[predName][predLabel]) + \"/\" + str(rowCount[\"ALL\"]) + \") * \"\n",
        "    a = p + \"(\"+calcH(predNameB,predLabelB,pd=rowCount[predName][predLabel])+\")\"\n",
        "    if (term == 0):\n",
        "      acc = a\n",
        "    else:\n",
        "      acc = acc + \" + \" + a\n",
        "    term += 1\n",
        "  H[className][predName] = eval(acc)\n",
        "\n",
        "  print(acc)\n",
        "  print(\"Splitting on {} gives entropy {}\".format(predName,H[className][predName]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66612fa0",
      "metadata": {
        "tags": [
          "choice"
        ]
      },
      "source": [
        "So which predictor should we use in our node, to split the data to get the smallest entropy over the available predictors?\n",
        "\n",
        "It seems that the `outlook` variable is the one to choose!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e29d8496",
      "metadata": {
        "lines_to_next_cell": 2,
        "tags": [
          "calcChoice"
        ]
      },
      "outputs": [],
      "source": [
        "splitVariable = min(H[className], key=H[className].get)\n",
        "print(\"The first split should be on the _{}_ variable, reducing the entropy from {} to {}\".format(splitVariable,H[className][\"\"],H[className][splitVariable]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "425482cd",
      "metadata": {
        "tags": [
          "discussionManual"
        ]
      },
      "source": [
        "Note that this process can be continued as necessary, with it stopping when all leafs are _pure_ (have entropy = 0). According to https://codefying.com/2015/03/09/decision-tree-classifier-part-1/, the resulting decision tree is shown below:\n",
        "\n",
        "![View of Play Tennis decision tree](https://codefying.files.wordpress.com/2015/03/mltree.jpg)\n",
        "\n",
        "Note that the `temp` predictor is not used in the decision tree - it is not needed."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6fd116a",
      "metadata": {
        "tags": [
          "discussionSklearn"
        ]
      },
      "source": [
        "## Using sklearn to derive the decision tree\n",
        "\n",
        "Note that the treatment above is intended to help understanding. There is no need to program Decision Tree classifiers yourself!\n",
        "\n",
        "`scikit-learn` offers a `DecisionTreeClassifier` with the same sort of API as other supervised learning algorithms, alongside settings that are specific to Decision Trees.\n",
        "\n",
        "One awkward feature of the sklearn implementation is that it seems to be necessary to code the labels as numbers (otherwise you get a \"could not convert string to float\" error). While there are tools to do this, it does mean that the resulting decision tree is not as easy to \"read\".\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55812e64",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn import tree\n",
        "features = ['outlook', 'temp', 'humidity', 'windy']\n",
        "X = df[features].copy()\n",
        "y = df['play'].copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19806233",
      "metadata": {},
      "source": [
        "Working with the copies X and y, we need to encode them to integers. Be careful when looking at code others have shared on the web. Older versions of sklearn did not have an OrdinalEncoder, so people used LabelEncoders instead. This usually led to encodings that did not respect the natural ordering of ordinal features. This is a problem for Decision Tress that are based on ordinal splits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ec4848a",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "le = LabelEncoder()\n",
        "oe = OrdinalEncoder(categories=\n",
        "                               [['rainy','overcast','sunny'],\n",
        "                                ['cool','mild','hot'],\n",
        "                                ['normal','high'],\n",
        "                                ['low','high']]).fit(X)\n",
        "oe"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d3528a6",
      "metadata": {},
      "source": [
        "Now apply the encodings to the variables, generating XE and yE from X and y respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "755f045b",
      "metadata": {},
      "outputs": [],
      "source": [
        "yE = pd.Series(le.fit_transform(y))\n",
        "XE = pd.DataFrame(data=oe.transform(X), columns=features)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5618eeea",
      "metadata": {},
      "source": [
        "Now look at the encoded features. Note that the encoded features respect the ordinal nature of the original features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2765606a",
      "metadata": {},
      "outputs": [],
      "source": [
        "XE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1af56adb",
      "metadata": {},
      "source": [
        "Now fit the data using the sklearn DecisionTree classifier, using the encoded features and labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7059beb5",
      "metadata": {},
      "outputs": [],
      "source": [
        "clf = tree.DecisionTreeClassifier()\n",
        "clf = clf.fit(XE, yE)\n",
        "clf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1418818",
      "metadata": {},
      "source": [
        "## Exercise\n",
        "\n",
        "Now that we have classified the data, here are some tasks for you\n",
        "\n",
        "1. Introduce an 80:20 train-test split and comment on the prediction error, size of data set, properties of decision trees, etc.\n",
        "2. Use the inverse_transform() method to make the tree easier to \"read\".\n",
        "3. Using [sklearn advice on using decision tree classifiers](https://scikit-learn.org/stable/modules/tree.html), investigate `export_graphviz()` and `export_text()` to export two ways to visualise the tree output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ae36371",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
